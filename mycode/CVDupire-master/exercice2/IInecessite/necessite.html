<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>La nécessité d’une analyse poussée face à l’ère du développement de l’IA</title>
    <link rel="stylesheet" href="../ex2.css">
</head>
<body>
    <h2>
        II. La nécessité d’une analyse poussée face à l’ère du développement de l’IA
    </h2>
    <h3>
        A. Une prise en compte de l’éthique dès la conception
    </h3>
    <h4>
        1. La nécessité de penser éthiquement l’usage de données personnelles dès la conception
    </h4>
    <p>
        Pour Maël Pégny, l’éthique s’envisage en amont de la conception notamment de l’IA pour y inclure les possibles
        dangers politiques qu’elle pourrait représenter de par la grande collection des données et leur centralisation ainsi que
        leur exploitation, qui est le point le plus problématique pour la vie privée. Il faut en outre prendre la mesure de ce
        qu’est une donnée au travers de son accès, sa qualité et notamment son intégrité.
        Force est de constater cependant que la résolution du problème éthique n’en est qu’à ses balbutiements. En
        témoigne le rapport de la commission Ethique du Ministère fédéral allemand des transports sur les véhicules
        autonomes et connectés, qui met en exergue l’impossibilité pour l’éthique et le juridique d’être inclus directement
        dans la programmation, qu’il faut cependant une approche de développement éthique qui parcourt l’ensemble du
        cycle de vie du logiciel afin d’en améliorer la conception.
        Cette idée pose cependant plusieurs problèmes notamment dans l’idée du développement éthique par essence,
        puisque les développeur.euses devront obtenir une expertise économique, juridique, sociologique et philosophique
        ce qui est une attente irréaliste pour Maël Pégny. Une acception plus réaliste serait une délimitation des difficultés
        de l’opérationnalisation, c'est-à-dire l’absence de limites à la création de l’IA qui est stimulée par la concurrence et
        le sensationnalisme. Cependant, le numérique et notamment l’IA ne peut plus être pensé uniquement sous un aspect
        technique. De par l’usage dont le numérique fait l’objet, il est nécessaire d’envisager l’éthique dès la conception du
        service ou du produit. L’intervenant Maël Pégny avait proposé une charte afin de s’assurer que le produit ou le service
        était éthique depuis sa conception.
        En effet, il évoque dans son écrit Pour un développement des IAs respectueux de la vie privée dès la conception7
        le fait de :
    </p>
    <ul>
        <li>déclarer la finalité de l’usage des données en documentant et justifiant tout écart à la déclaration initiale,
        </li>
        <li>
            tester les performances finales du logiciel et limiter le pouvoir prédictif du modèle à ce qui est nécessaire,
        </li>
        <li>
            mettre en balance risque de suroptimisation et de perte de performances en prenant en compte la vie
            privée,
        </li>
        <li>
            entraîner si possible son modèle sans avoir recours aux données personnelles, ou d’utiliser des données
            ayant fait l’objet d’un geste explicite de publication, la mise à jour des données, retrait de publication et
            l’exercice des droits de rectification, d’effacement et d’opposition,
        </li>
        <li>
            déclarer les raisons justifiant l’utilisation des données personnelles ainsi que les mesures prises pour lutter
            contre la rétro-ingénierie des données, et prendre position sur leur complétude par rapport aux boîtes à
            outils et aux méthodes d’attaque existantes,
        </li>
        <li>
            diffuser en licence libre tous les outils de sécurisation contre la rétro-ingénierie des données,
        </li>
        <li>
            mettre le modèle à disposition de tous afin de permettre la vérification publique des propriétés de sécurité
            si cela n'entraîne pas de faille de sécurité intolérable et le justifier,
        </li>
        <li>
            lorsque les mesures de restrictions de la collecte et de lutte contre la rétroingénierie ne sont pas applicables
            et que la gravité de l’enjeu surpasse les enjeux de vie privée,
        </li>
        <li>
            autoriser un modèle encodant des données privées en restreignant strictement l’accès à ce modèle et son
            emploi pour l’usage ayant justifié l’exception.
        </li>
    </ul>
    <p>
        Cependant, en l’état actuel du droit, la législation a apporté un lot non négligeable de sécurisation et d’éthique du
        traitement des données qui constitue un commencement à cet idéal de protection qui se dessine au travers de la charte
        telle que conçue par Maël Pégny.
    </p>
    
    <h4>
        2. L’état du droit
    </h4>
    <p> 
        En effet, le droit de l’Union Européenne envisage la protection des données comme des droits et se réfère aux
        articles 7 et 8 de la Charte des droits fondamentaux de l’Union, à l’article 16, § 1er, TFUE, ainsi qu’à l’article 8 de
        la Convention européenne des droits de l’homme et dans la Convention n° 108 du Conseil de l’Europe, le RGPD et
        le règlement « Vie privée et communications électroniques » de la Communication de 2017 de la Commission
        européenne remplaçant la directive 2002/58/CE. Cet ensemble de textes législatifs encadrent et protègent la
        protection de la vie privée et sanctionnent sévèrement ceux qui commettraient un manquement. Par ailleurs, le
        Règlement Général sur la Protection des Données s’apparente à la Charte qu’imagine Maël Pégny. En effet, ce
        règlement impose de justifier de la finalité du traitement, la possibilité de retirer son consentement, corriger les
        données, sécuriser les données par des moyens techniques. Le règlement met également en place un principe de
        minimisation.
        Le principe de minimisation selon la Cnil « prévoit que les données à caractère personnel doivent être adéquates,
        pertinentes et limitées à ce qui est nécessaire au regard des finalités pour lesquelles elles sont traitées. » Maël Pégny
        dans sa conférence et son écrit émet la critique selon laquelle la finalité peut être ajustée afin d’obtenir de la façon la
        plus extensive possible les données personnelles, que les libertés accordées à la recherche par exemple sont modifiées
        au fil de l’eau. A l’occasion d’un traitement, il convient de définir la finalité du traitement, puisque c’est essentiel
        dans l’exploitation des données personnelles et leur protection. Cette finalité peut en effet être difficile à délimiter si
        elle concerne la recherche scientifique, celle-ci pouvant être élargie ou précisée.
        L’exigence de finalité seule n’est pas suffisante. Cependant force est de constater que pareillement à la Charte de
        Maël Pégny, le RGPD soumet tout traitement de données à caractère personnel, incluant la recherche scientifique et
        exige la documentation, la justification ainsi que le consentement du propriétaire des données avant toute
        modification du traitement ou de sa finalité. Pour la recherche scientifique en matière de santé, certaines dérogations
        existent. C’est notamment le cas au sujet de la définition de la finalité de ce traitement. Cette définition peut être plus
        générale s’il n’existe pas de façon plus précise de la délimiter, mais doit demeurer. La CNIL contrôle le traitement
        des données de façon stricte mais cela n’est évidemment pas le cas de tous les pays, ce qui pose un problème de
        territorialité des règlements. La limite de la législation se situe au-delà des frontières physiques des pouvoirs
        législatifs et également au début du domaine plus technique du numérique, notamment dans les algorithmes, que le
        droit n’a pas encore saisi. Le traitement automatisé ou non automatisé avec ou sans des outils logiciels est tombé
        sous le coup de la loi. En revanche, il est essentiel de sensibiliser aux questions éthiques dès la conception des produits
        et services. En effet, une éthique en amont permet aux législateurs, en collaboration avec les experts de la technique
        numérique, d’élaborer une législation au plus proche des besoins éthiques des outils numériques dans le traitement
        des données personnelles ainsi que d’étendre la territorialité de la protection des données.
        Dès lors, l’élaboration de chartes de la part des experts serait une collaboration avec le législateur.
    </p>
    <h4>
        3. L’objet d’une charte développement éthique
    </h4>
    <p>La charte telle qu’imaginée par Maël Pégny est une éthique du développement et de son impact. En l’occurrence,
        cette charte se spécialise sur la protection des données personnelles et de la vie privée, plus précisément la vie privée
        par la collecte massive de données nécessaire au développement, mais aussi par l’enregistrement, et la possible
        récupération de données à partir des modèles. Selon Maël Pégny, l’utilisation d’une charte est d’autant plus
        importante dans les algorithmes et il vise plus spécifiquement les modèles d’apprentissage automatique, qui seront
        selon lui un « point de tension entre le désir et le respect de la vie privée et les idéaux de diffusion libre du logiciel
        et de reproductibilité de la recherche ». Selon lui, la valeur de l’information privée provient de son caractère restreint,
        et sa diffusion constituerait une « dégradation et non une ouverture à un épanouissement collectif ». Dès lors, elle se
        doit de rester privée autant que possible. 
        L’utilisation d’une charte telle que celle de Maël Pégny serait également une prise de conscience, un engagement
        à titre personnel de conformité avec l’état actuel du droit, les enjeux éthiques et les questions de sécurité. La charte
        est un moyen de communication des principes éthiques sur lequel les modèles sont créés. C’est un début de stratégie,
        une politique d’orientation dans la façon d’utiliser les données personnelles et une collaboration avec le législateur
        qui n’a pas l’expertise de saisir les enjeux dans les domaines techniques numériques mais qui pourrait se saisir des
        chartes qui prendraient en mesure l’éthique et ainsi s’approprier ces valeurs pour les traduire en règles contraignantes
        et impératives.
    </p>
    <h3>
        B. L’éthique de la surveillance

    </h3>
    <h4>
        1. La surveillance étatique

    </h4>
    <p>

        Selon Bruce Schneider, nous sommes dans un “âge d’or de la surveillance”, les institutions n’ayant jamais eu
        autant de données sur les individus ni de moyens de surveillance continue. Cette surveillance n’est pas uniquement
        marketing comme le souligne S. Zuboff en parlant de capitalisme de la surveillance. En ce sens, la Commission
        européenne dans une note blanche du 19 février 2020 Une approche européenne axée sur l'excellence et la confiance
        met en garde contre « la mise en place de barrières et d’exigences lourdes qui peuvent constituer un obstacle à
        l’innovation ». Au lieu de cela, ces 14 ministères suggèrent que l’Europe « devrait se tourner vers des solutions
        juridiques non contraignantes telles que l’autorégulation, la labellisation volontaire et d’autres pratiques » de ce type.
        L’anonymat tend également à décroître du fait de l’archivisme et de la capacité d’identifier par croisement de données
        massives, de la surveillance dans l’espace public. Il y a deux écoles par rapport à cette surveillance. La première est
        celle de la réglementation que décrit l’ouvrage The partial Line qui défend un usage limité de la surveillance pour le
        terrorisme par exemple ou des disparitions, position que choisit l’UE. La seconde, est celle de la prohibition, c’est
        l’idée que cela ne devrait pas exister.
        La surveillance est encadrée par l’Union Européenne au travers de textes législatifs, que conteste Maël Pégny.
        L’Union Européenne afficherait une ambition de donner une définition des applications de l’IA à haut risque (santé,
        sécurité, droits fondamentaux) et l’identification de caractéristiques de la technologie (opacité, complexité,
        dépendance aux données, comportement autonome). Selon lui, on cherche à créer des bacs à sable pour tester des
        règlements.
        L’Union européenne interdit la mise en place de « techniques subliminales agissant sur l’inconscient »,
        l’exploitation de vulnérabilités de groupes vulnérables spécifiques (handicapés, enfants), pouvant causer un préjudice
        psychologique ou physique et enfin la prise en compte de certains excès des autorités publiques (sauves usages
        militaires, coopération internationale de services): notation sociale fondée sur l’IA à des fins générales et exige une
        transparences sur les interactions avec un robot, détection d’émotions, associations avec des catégories sociales sur
        la base de données biométriques, trucages vidéo ultra réalistes. Cette surveillance embarque également le
        développement de LFRT, un impact conceptuel sur la notion de DCP, la fin du paradigme de protection par restriction
        d’accès, remet en cause la distinction donnée-programme/algorithme, de se pencher sur le rapport traitement et
        données.
        L’Union Européenne et notamment la Cour Européenne des Droits de l'Homme ainsi que la CJUE se sont pourtant
        emparées de ces questions, avec opposition entre ces deux institutions, la Cour de Justice étant plus protectrice en
        l'occurrence des droits fondamentaux. Elles se reposent sur l’article 8 de la Convention Européenne de Sauvegarde
        des Droits de l’Homme qui protège la vie privée. Il est cependant apparu que les institutions nationales et européennes
        sont assez permissives au niveau de la surveillance de masse sous couvert de protection antiterroriste en témoigne un
        arrêt rendu par la CJUE Télé 2 du 21 décembre 2014 qui détermine la nécessité d’encadrer strictement la conservation
        des données et leur accès, et détermine le fait d’organiser une conservation générale des données comme
        incompatibles avec les articles 7 et 8 de la Convention de Sauvegarde des Droits de l’Homme. Il convient également
        de permettre plus de transparence sur les algorithmes, droit mis en place par le RGPD mais très rarement exercé, et
        les administrations sont réticentes à donner l’algorithme utilisé en témoigne les scandales autour de la mise en place
        de Parcoursup.
    </p>
    <h4>
        2. Les algorithmes discriminatoires
    </h4>
    <p>

        La raison pour laquelle les algorithmes peuvent être discriminatoires est l’utilisation de données biaisées. Le biais
        le plus fréquent serait le manque de représentativité des données mobilisées dans un système d’apprentissage
        automatique si celui-ci est la traduction de pratiques et comportements. Par exemple, dans les données d’emploi les
        femmes sont moins représentées et occupent des filières de métiers ainsi que des postes à rémunérations moindres.
        L’algorithme pourrait en déduire que les femmes sont moins productives que les hommes et ne vont pas vers des
        postes à responsabilité et accentuerait le recrutement des hommes sur cette base. Quand bien même des
        caractéristiques neutres en apparence seraient utilisées, cela peut avoir des effets discriminatoires comme l’a souligné
        le Défenseur des droits dans sa décision Parcoursup qui a mis en relief que l’utilisation de la donnée de l’établissement
        d’origine compromettait la mixité sociale et l’ascension des jeunes défavorisés.
        Les algorithmes avec des biais pourraient également remettre en cause la liberté d’expression. Maël Pégny prend
        en exemple la loi visant à lutter contre les contenus haineux sur internet adoptée par l’Assemblée Nationale le 13 mai
        2020 destinée à retirer certains contenus haineux sous 24 h des réseaux sociaux, des plates-formes collaboratives et
        des moteurs de recherche. L'utilisation de procédés algorithmiques sur la base de mots clés pourraient entraîner une
        censure excessive et devenir contreproductive.
        On peut concevoir des débuts de solutions en soutenant la recherche afin de développer les études de mesures
        ainsi que des méthodologies de prévention des biais. Cumulativement, il serait opportun de réaliser des études
        d’impacts afin d’anticiper les effets discriminatoires des algorithmes, et cela a par ailleurs été initié dans un tableau
        d’exemples théorique de l’impact de l’IA et des décisions automatisées Examples of theoretical assessment of harm
        and significant impact of AI or automated decisions de 2020. Des organisations et institutions commencent à
        s’intéresser à la prévention des discriminations dans les algorithmes, en témoignent les travaux de l’institut
        international ombudsman en partenariat avec la CNIL : Algorithme : prévenir l’automatisation des discriminations,
        qui a eu lieu pendant la crise sanitaire. Il faut également citer le rapport de l’Agence des droits fondamentaux de
        l’Union européenne qui ouvre la discussion de l’éthique et des droits fondamentaux Bien préparer l’avenir :
        l’intelligence artificielle et les droits fondamentaux du 14 décembre 2020.
    </p>
    <h3>
        C. Les approches juridiques et philosophiques
    </h3>
    <h4>
        1. L’inapplicabilité de la dichotomie données-logiciels en matière de modèles d’apprentissage automatique
    </h4>
    <p>
        Conséquence juridique prééminente des problèmes de sécurité des modèles, la dichotomie données-logiciels
        pose aujourd’hui des problèmes d’éparpillement du droit en matière de données. En effet, l’avènement des modèles
        d’apprentissage automatique (Mode Machine Learning) ont ceci de particulier que les données sont encodées comme
        un logiciel et ainsi, sont soumises à un régime différent. Bien que la dichotomie posait déjà problème avant les
        modèles d’apprentissage automatique dans la mesure où les traitements étaient déjà possiblement invertibles8
        , la
        différence de régime au sein de la législation européenne entre données brutes et données traitées fut renforcée. Du
        fait du caractère fondamentalement indéterminé et évolutif des concepts juridiques ou comme l’énonce le philosophe
        H.Hart de la « texture ouverte du droit »9
        , le raisonnement de cette science molle autour d’un possible régime
        juridique en matière d’IA pose de nombreux problèmes face aux ambiguïtés sémantiques et syntaxiques des différents
        textes juridiques. Face au droit de la protection des données personnelles, les droits de la propriété intellectuelle et le
        secret commercial s’exercent de façon prépondérante. 
        Toutefois, la réelle question à se poser est la suivante : quel est l'enjeu de la définition d’un régime juridique sûr
        et stable ? En effet, l’application distributive de différents régimes juridiques s’exercent dans différents domaines
        sans poser aujourd’hui de réelles difficultés. Cependant, en matière d’IA, l’enjeu étant de protéger les données
        notamment les données personnelles, la fixation d’un régime juridique, bien que pouvant évoluer avec le temps, est
        aujourd’hui primordiale. Il existe des interférences avec la dichotomie données-logiciels comme l’énonce le
        Professeur Michael Veale10 au sein de son ouvrage Algorithms that remember: model inversion attacks and data
        protection law,. C’est notamment le cas des modèles susceptibles d’une attaque par inférence d’appartenance11 ou
        par inversion portant sur des données pseudonymisées ou soumises à un autre traitement cryptographique. Une telle
        analogie vient faire exploser la barrière entre données traitées et données brutes. Il convient tout de même de noter
        que l’état actuel du droit n’est pas étranger à cet éclatement. En effet, les données pseudonymisées ont bien fait l’objet
        d’un traitement, qui plus est d’un traitement explicitement conçu pour protéger les droits des sujets de données.
        Toutefois, celles-ci sont soumises au même statut juridique que les données personnelles dont elles sont tirées et
        constituent déjà une exception au cadre juridique général des données traitées. Cette problématique autour des
        données personnelles est notamment appréhendée par le Règlement du Parlement européen et du Conseil relatif à la
        protection des personnes physiques à l'égard du traitement des données à caractère personnel et à la libre circulation
        de ces données (ci-après « RGPD ») notamment par la technique de probabilité de ré-identification12, partie présente
        de l’identification de la notion de « caractère personnel » des données13
        Les débats entourant la notion de la patrimonialisation de la donnée14 sont aussi au cœur de notre sujet. En effet,
        la notion de la propriété par le droit de la propriété intellectuelle et le secret d’affaire est envisagée sous le spectre
        d’une propriété privée par le travail et l’investissement financier, en opposition avec le droit de la protection des
        données, véritable avancée vers un droit fondé sur le rapport à soi. Ainsi reprenant les termes de M. Pégny : « lorsque
        le droit des données personnelles se voit donner prééminence sur la propriété intellectuelle ou le secret des affaires,
        l’intuition que la vie privée constitue un droit fondamental écrasant le droit de propriété, ou constituant un droit de
        propriété privilégié » est encensée. En d’autres termes, « je suis le propriétaire de mes données, même si c’est vous
        qui travaillez avec et sur elles ».
        Outre l’inapplicabilité de la dichotomie données-logiciels, celle des données anonymes/personnelles doit être
        envisagée.
    </p>
    <h4>
        2. L’inapplicabilité de la dichotomie données personnelles-anonymes en matière de modèles
        d’apprentissage automatique
    </h4>
    <p>
        Selon le Considérant 26 du RGPD les données anonymes sont les données dépourvues de référence à une
        personne physique, ou traitées de telle manière que les personnes physiques ne soient plus identifiables. En effet, la
        difficulté pour toute entreprise est de garantir que les données collectées, ayant fait l’objet d’une anonymisation ne
        peuvent plus, par recoupement, contribuer à la ré-identification d’une personne physique. La notion d’identification
        directe employée au sein du Considérant 26 est définie comme « l’identification par le nom propre, éventuellement
        secondée par une information distinguant les homonymes, tandis que l’identification indirecte est définie par une
        combinaison unique d’identifiants permettant de singulariser l’individu au sein d’un groupe ». Comme l’énonce le
        Working Party 2915 (G29), aujourd’hui remplacé par le Comité Européen sur la Protection des Données16 lors de
        l’Opinion 4/2007 On the concept of personal data17 : « une donnée n’est anonyme que lorsque l’anonymisation est 
        irréversible, c’est-à-dire quand il n'est pas possible de retrouver l’identité d’une personne physique à partir de cette
        donnée avec les moyens existants de la technologie. Cette possibilité de ré-identification peut en outre être comprise
        dans un sens technique absolu, ou non dans un sens relatif aux moyens à la disposition du contrôleur de données ».
        Ainsi, comme l’énonce le Professeur Nadezdha Purtova18, une grande partie des difficultés éprouvées quant à
        cette définition provient de sa souplesse mais aussi de l’introduction au fil du temps de modalités supplémentaire en
        raison des nombreuses évolutions technologiques, continuant à modifier les conditions sous lesquelles une personne
        peut être identifiable19. L’avancement technologique remettant sans cesse en question les contours de la définition de
        donnée anonyme, un mouvement de datafication20 discrète du concept de donnée personnelle fait aujourd’hui son
        œuvre, entraînant une dépendance contextuelle du statut de données personnelle à l’évolution de l'état de l’art
        technologique.
        Dans ce cadre, les conditions d’objectif et de résultat donnent une portée potentiellement très ample au concept
        de donnée personnelle. Cette vaste extension de la notion peut être justifiée par le fait que les données dont le contenu
        fait directement référence à un individu ne sont pas les seules à pouvoir causer un tort informationnel.
        Enfin, l’interprétation maximaliste du droit des données mènerait à son inapplicabilité en raison de son approche
        par la catégorisation, au détriment de celle par la réglementation de l’usage des connaissances déduites sur les
        personnes concernées. Selon une autre perspective critique défendue par les Professeurs Wachter et Mittelstadt au
        sein de de l’article A right to reasonable inference21, le droit actuel n’offre qu’une protection imparfaite dans un
        monde où la distinction nette entre données et logiciel ne peut plus constituer les critères de constitution des
        législations existantes et futures, devant au contraire être recentré sur la notion d’inférence raisonnable 22. Une
        approche par l’usage et non le résultat serait, selon eux, la solution. L’applicabilité d’une telle approche aurait pour
        mérite de dissoudre les problèmes de catégorisation posés, dont les modèles peuvent être considérés à la fois comme
        des données et comme des logiciels au titre du droit actuel.
    </p>
    <br>
    <a class="retour_accueil" href="../ex2.html">Retour à l'accueil</a>
    
</body>
</html>